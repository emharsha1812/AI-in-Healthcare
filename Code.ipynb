{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "PROCESSED_PATH = \"./Data/ICDAR_SROIE/processed/\"\n",
    "\n",
    "# Loading ocr and label data\n",
    "receipt_train_img = {os.path.split(x)[-1].replace(\".jpg\",\"\"):x for x in glob.glob(\"./Data/ICDAR_SROIE/0325updated.task1train(626p)/*.jpg\") if not os.path.split(x)[-1].replace(\".jpg\",\"\").endswith(\")\")}\n",
    "\n",
    "ocr_data = {os.path.split(x)[-1].replace(\".txt\",\"\"):x for x in glob.glob(\"./Data/ICDAR_SROIE/0325updated.task1train(626p)/*.txt\") if not os.path.split(x)[-1].replace(\".txt\",\"\").endswith(\")\")} \n",
    "label_data = {os.path.split(x)[-1].replace(\".txt\",\"\"):x for x in glob.glob(\"./Data/ICDAR_SROIE/0325updated.task2train(626p)/*.txt\") if not os.path.split(x)[-1].replace(\".txt\",\"\").endswith(\")\")}\n",
    "\n",
    "# Checking if all the sets have the same number of labelled data\n",
    "assert len(receipt_train_img) == len(ocr_data) == len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from fuzzywuzzy import fuzz\n",
    "def extract_ocr_data_fromtxt(file_path, key, save = False):\n",
    "    \"\"\"\n",
    "    Extract the bounding box coordinates from txt and returns a pandas dataframe\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as in_file:\n",
    "        stripped = (line.strip() for line in in_file)\n",
    "        lines = [line.split(\",\")[:2] + line.split(\",\")[4:6] + [\",\".join(line.split(\",\")[8:])] for line in stripped if line]\n",
    "        \n",
    "        df = pd.DataFrame(lines, columns = ['xmin', 'ymin','xmax', 'ymax','text'])\n",
    "        # Option to save as a csv\n",
    "        if save:\n",
    "            if not os.path.exists(PROCESSED_PATH):\n",
    "                os.mkdir(PROCESSED_PATH)\n",
    "            df.to_csv(os.path.join(PROCESSED_PATH,key + '.csv'), index =None)\n",
    "        return df\n",
    "            \n",
    "def extract_label_data_fromtxt(file_path):\n",
    "    \"\"\"\n",
    "    Read the label json and return as a dictionary\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        json_data = json.load(f)\n",
    "        return json_data\n",
    "    \n",
    "def map_labels(text,k):\n",
    "    \"\"\"\n",
    "    Maps label to ocr output using certain heuristics and logic\n",
    "    \"\"\"\n",
    "    text_n = None\n",
    "    k_n = None\n",
    "    try:\n",
    "        text_n = float(text)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        k_n = float(k)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    # if both are text then we are doing a fuzzy match\n",
    "    if (pd.isnull(text_n) and pd.isnull(k_n)):\n",
    "#         if (text in k) or (k in text):\n",
    "#             return True\n",
    "        if fuzz.token_set_ratio(text,k) > 90:\n",
    "            return True\n",
    "    # if both are numerical then we just check for complete match\n",
    "    elif (text_n is not None) and (k_n is not None):\n",
    "        return text == k\n",
    "    # special case to handle total, using endwith \n",
    "    # as sometimes symbols are attached to ocr output\n",
    "    elif (k_n is not None) and (text_n is None):\n",
    "        return text.endswith(k)\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped_label_ocr(key):\n",
    "    \"\"\"\n",
    "    Wrapper function to yield result of mapping in desired format\n",
    "    \"\"\"\n",
    "    data = extract_ocr_data_fromtxt(ocr_data[key],key)\n",
    "    label_dict = extract_label_data_fromtxt(label_data[key])\n",
    "    \n",
    "    data['labels'] = [[k for k,v in label_dict.items() if map_labels(text, v)] for text in data.text]\n",
    "    # To avoid company and address overlap in some cases.\n",
    "    data['labels'] = ['address' if len(label) > 1 else \"\".join(label) for label in data['labels']]\n",
    "    \n",
    "    if not os.path.exists(PROCESSED_PATH):\n",
    "        os.mkdir(PROCESSED_PATH)\n",
    "    data.to_csv(os.path.join(PROCESSED_PATH,key + '.csv'), index =None)\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data = {key: mapped_label_ocr(key) for key in ocr_data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot some of these labels and see the results\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "LABELLED_IMG = \"./Data/ICDAR_SROIE/labelled_img/\"\n",
    "if not os.path.exists(LABELLED_IMG):\n",
    "    os.mkdir(LABELLED_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# image = Image.open(receipt_train_img['X00016469612'])\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_labels(key, df, save_img = False):\n",
    "    \"\"\"returns invoices with manually annontated labels\"\"\"\n",
    "    \n",
    "    img = cv2.imread(receipt_train_img[key])\n",
    "\n",
    "\n",
    "    for i, rows in df.iterrows():                   \n",
    "            text = rows['labels'].upper() \n",
    "\n",
    "            x1,y1,x3,y3 = int(rows['xmin']),int(rows['ymin']),int(rows['xmax']),int(rows['ymax'])\n",
    "\n",
    "            img = cv2.rectangle(img = img, \n",
    "                                pt1 = (x1, y1), \n",
    "                                pt2 = (x3, y3), \n",
    "                                color = (255, 0, 0),\n",
    "                               thickness = 1)\n",
    "            img = cv2.putText(img, text, (x1, y1 - 1),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 0, 255), 1)\n",
    "\n",
    "    if save_img:\n",
    "        cv2.imwrite(LABELLED_IMG + key + '_withlabels.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_correct_data = {os.path.split(x)[-1].replace(\".csv\",\"\") : pd.read_csv(x, index_col = None) for x in glob.glob(\"./Data/ICDAR_SROIE/processed_correct/*.csv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in mapped_correct_data.keys():\n",
    "    df = mapped_data[key].dropna().reset_index(drop = True)\n",
    "#     df = mapped_correct_data[key].dropna().reset_index(drop = True)\n",
    "    visualize_labels(key, df, save_img = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_TEXT_PATH = \"./Data/ICDAR_SROIE/processed_text_features\"\n",
    "\n",
    "if not os.path.exists(PROCESSED_TEXT_PATH):\n",
    "    os.mkdir(PROCESSED_TEXT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import collections\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "from itertools import groupby\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def get_text_features(text):\n",
    "    \n",
    "    # SpecialCharacterCount  \n",
    "    special_chars = string.punctuation\n",
    "    SpecialCharacterCount = np.sum([v for k, v in collections.Counter(text).items() \\\n",
    "                  if k in special_chars])\n",
    "    \n",
    "    # isFloat \n",
    "    try:\n",
    "        float(text)\n",
    "        isFloat = 1\n",
    "    except Exception as e:\n",
    "        isFloat = 0\n",
    "        \n",
    "    # isDate \n",
    "    try: \n",
    "        parse(text, fuzzy=True)\n",
    "        isDate = int(True and len(text) > 5)\n",
    "    except Exception as e:\n",
    "        isDate = 0\n",
    "    \n",
    "    # TotalDistinctNumber\n",
    "    num_list = re.findall(r\"(\\d+)\", text)\n",
    "    num_list = [float(x) for x in num_list]\n",
    "    \n",
    "    TotalDistinctNumber = len(num_list)\n",
    "    \n",
    "    # BigNumLength \n",
    "    BigNumLength = np.max(num_list) if TotalDistinctNumber > 0 else 0\n",
    "    \n",
    "    \n",
    "    # DoesContainsNum \n",
    "    DoesContainsNum = 1 if TotalDistinctNumber > 0 else 0\n",
    "    \n",
    "    # POSTagDistribution \n",
    "    spacy_text = nlp(text)\n",
    "    pos_list = [token.pos_ for token in spacy_text]\n",
    "    \n",
    "    POSTagDistribution = {}\n",
    "    for k in ['SYM','NUM','CCONJ','PROPN']:\n",
    "        POSTagDistribution['POSTagDistribution' + k] = [0]\n",
    "        \n",
    "    POSTagDistribution.update({'POSTagDistribution'+ value: [len(list(freq))] for value, freq in groupby(sorted(pos_list)) if value in ['SYM','NUM','CCONJ','PROPN']})\n",
    "    \n",
    "    pos_features = pd.DataFrame.from_dict(POSTagDistribution)\n",
    "    other_features = pd.DataFrame([[SpecialCharacterCount, isFloat, isDate,\n",
    "                                  TotalDistinctNumber, BigNumLength, DoesContainsNum]], \n",
    "                                  columns = [\"SpecialCharacterCount\",\"isFloat\",\"isDate\",\n",
    "                                                \"TotalDistinctNumber\",\"BigNumLength\", \"DoesContainsNum\"])\n",
    "    \n",
    "    df = pd.concat([other_features, pos_features], axis = 1)\n",
    "    return df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname PRINT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname JALAN identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname JALAN identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname S identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname JALAN identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname JALAN identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname JALAN identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname S identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname B identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname COVER identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname COVER identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname JALAN identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname JALAN identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname LEVEL identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname TO identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname F identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname WIRE identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname S identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname S identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname PLUG identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname OASIS identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "mapped_data_text_features = {}\n",
    "for k, v in mapped_data.items():\n",
    "    _df = pd.concat([get_text_features(x) for x in v.text], axis = 0)\n",
    "    final_df = pd.concat([v.reset_index(drop = True), _df.reset_index(drop = True)], axis = 1)\n",
    "    final_df.to_csv(os.path.join(PROCESSED_TEXT_PATH,k+\".csv\"), index = None)\n",
    "    mapped_data_text_features[k] = final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_line_numbers(key):\n",
    "    \"\"\"\n",
    "    Get line number for each word.\n",
    "    \"\"\"\n",
    "    ################ 1 ##################\n",
    "    \n",
    "    df = mapped_data_text_features[key]\n",
    "    df.sort_values(by=['ymin'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # To avoid spacing issue, lets reduce ymax by some small value\n",
    "    df[\"ymax\"] = df[\"ymax\"].apply(lambda x: int(x) - 0.5)\n",
    "    \n",
    "    \n",
    "    ################ 2 ##################\n",
    "    # In order to get line number we start with left most word/phrase/node \n",
    "    # and then check all non-matching words and store their indices from L->R\n",
    "    word_idx = []\n",
    "    for i, row in df.iterrows():\n",
    "        flattened_word_idx = list(itertools.chain(*word_idx))\n",
    "        # check if the word has not already been checked\n",
    "        if i not in flattened_word_idx:\n",
    "            top_wa = int(row['ymin'])\n",
    "            bottom_wa = int(row['ymax'])\n",
    "\n",
    "            # Store the word\n",
    "            idx = [i]\n",
    "        \n",
    "            for j, row_dash in df.iterrows():\n",
    "                if j not in flattened_word_idx:\n",
    "                # check a different word, double check\n",
    "                    if not i == j:\n",
    "                        top_wb = int(row_dash['ymin'])\n",
    "                        bottom_wb = int(row_dash['ymax'] )\n",
    "                        # Valid for all the words next to Wax\n",
    "                        if (top_wa <= bottom_wb) and (bottom_wa >= top_wb): \n",
    "                            idx.append(j)\n",
    "            word_idx.append(idx)\n",
    "    \n",
    "    # Create line number for each node\n",
    "\n",
    "    word_df = pd.DataFrame([[j,i+1] for i,x in enumerate(word_idx) for j in x], columns= [\"word_index\",\"line_num\"])\n",
    "    \n",
    "    # put the line numbers back to the list\n",
    "    final_df = df.merge(word_df, left_on=df.index, right_on='word_index')\n",
    "    final_df.drop('word_index', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    ################ 3 ##################\n",
    "    final_df = final_df.sort_values(by=['line_num','xmin'],ascending=True)\\\n",
    "            .groupby('line_num').head(len(final_df))\\\n",
    "            .reset_index(drop=True)\n",
    "    final_df['word_id'] = list(range(len(final_df)))\n",
    "\n",
    "\n",
    "    return final_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data_text_features_line = {key:get_line_numbers(key) for key,_ in mapped_data_text_features.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_IMAGE_PATH = \"./Data/ICDAR_SROIE/processed_graph_images\"\n",
    "\n",
    "if not os.path.exists(GRAPH_IMAGE_PATH):\n",
    "    os.mkdir(GRAPH_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkData():\n",
    "    def __init__(self, final_connections, G, df):\n",
    "        self.final_connections = final_connections\n",
    "        self.G = G\n",
    "        self.df = df\n",
    "    def get_connection_list():\n",
    "        return self.final_connections\n",
    "    def get_networkx_graph():\n",
    "        return self.G\n",
    "    def get_processed_data():\n",
    "        return self.df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def graph_modelling(key, save_graph =False):\n",
    "\n",
    "    # Horizontal edge formation   \n",
    "   \n",
    "    df = mapped_data_text_features_line[key]\n",
    "    df_grouped = df.groupby('line_num')\n",
    "    \n",
    "    # for directed graph\n",
    "    left_connections = {}    \n",
    "    right_connections = {}\n",
    "\n",
    "    for _,group in df_grouped:\n",
    "        wa = group['word_id'].tolist()        \n",
    "        #2\n",
    "        # In case of a single word in a line this will be an empty dictionary\n",
    "        _right_dict = {wa[i]:{'right':wa[i+1]} for i in range(len(wa)-1) }\n",
    "        _left_dict = {wa[i+1]:{'left':wa[i]} for i in range(len(wa)-1) }\n",
    "\n",
    "\n",
    "        #add the indices in the dataframes\n",
    "        for i in range(len(wa)-1):\n",
    "            df.loc[df['word_id'] == wa[i], 'right'] = int(wa[i+1])\n",
    "            df.loc[df['word_id'] == wa[i+1], 'left'] = int(wa[i])\n",
    "    \n",
    "        left_connections.update(_left_dict)\n",
    "        right_connections.update(_right_dict)\n",
    "\n",
    "    # Vertical edge formation\n",
    "    bottom_connections = {}\n",
    "    top_connections = {}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if i not in bottom_connections.keys():\n",
    "            for j, row_dash in df.iterrows():\n",
    "\n",
    "                # since our dataframe is sorted by line number and we are looking for vertical connections\n",
    "                # we will make sure that we are only searching for a word/phrase next in row.\n",
    "                if j not in bottom_connections.values() and i < j:\n",
    "                    if row_dash['line_num'] > row['line_num']: \n",
    "                        bottom_connections[i] = j\n",
    "\n",
    "                        top_connections[j] = i\n",
    "\n",
    "                        #add it to the dataframe\n",
    "                        df.loc[df['word_id'] == i , 'bottom'] = j\n",
    "                        df.loc[df['word_id'] == j, 'top'] = i \n",
    "\n",
    "                        # break once the condition is met\n",
    "                        break \n",
    "\n",
    "\n",
    "    # Merging Neighbours from all 4 directions \n",
    "    final_connections = {}\n",
    "    \n",
    "    # Taking all the keys that have a connection in either horizontal or vertical direction\n",
    "    # Note : Since these are undirected graphs we can take either of (right, left) OR (top, bottom)\n",
    "    for word_ids in (right_connections.keys() | bottom_connections.keys()):\n",
    "        if word_ids in right_connections: final_connections.setdefault(word_ids, []).append(right_connections[word_ids]['right'])\n",
    "        if word_ids in bottom_connections: final_connections.setdefault(word_ids, []).append(bottom_connections[word_ids])\n",
    "\n",
    "    # Create a networkx graph for ingestion into stellar graph model\n",
    "    G = nx.from_dict_of_lists(final_connections)\n",
    "    \n",
    "    # Adding node features\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(df[['SpecialCharacterCount', 'isFloat', 'isDate', 'TotalDistinctNumber',\n",
    "       'BigNumLength', 'DoesContainsNum', 'POSTagDistributionSYM',\n",
    "       'POSTagDistributionNUM', 'POSTagDistributionCCONJ',\n",
    "       'POSTagDistributionPROPN', 'line_num']])\n",
    "    node_feature_map = {y:x for x,y in zip(scaled_features, df.word_id)}\n",
    "    \n",
    "    for node_id, node_data in G.nodes(data=True):\n",
    "        node_data[\"feature\"] = node_feature_map[node_id]\n",
    "\n",
    "    if save_graph:\n",
    "        # There are multiple layouts but KKL is most suitable for non-centric layout\n",
    "        layout = nx.kamada_kawai_layout(G) \n",
    "        \n",
    "        # Plotting the Graphs\n",
    "        plt.figure(figsize=(10,5))\n",
    "        # Get current axes\n",
    "        ax = plt.gca()\n",
    "        ax.set_title(f'Graph form of {key}')        \n",
    "        nx.draw(G, layout, with_labels=True)\n",
    "        plt.savefig(os.path.join(GRAPH_IMAGE_PATH, key +\".jpg\"), format=\"JPG\")\n",
    "        plt.close()\n",
    "    \n",
    "    networkobject = NetworkData(final_connections, G, df)\n",
    "    return networkobject \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_net_obj = {key: graph_modelling(key, save_graph=True) for key,_ in mapped_data_text_features_line.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = nx.union_all([obj.G for k,obj in mapped_net_obj.items()], rename=[k+\"-\" for k in mapped_net_obj.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stellargraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstellargraph\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msg\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstellargraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullBatchNodeGenerator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstellargraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCN\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stellargraph'"
     ]
    }
   ],
   "source": [
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sg = sg.StellarGraph.from_networkx(U, node_features=\"feature\")\n",
    "print(G_sg.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = pd.DataFrame([[k+\"-\"+str(node_idx), label] \n",
    "                 for k,obj in mapped_net_obj.items()\\\n",
    "                for node_idx,label in zip(obj.df.word_id,obj.df.labels)],\n",
    "                             columns = [\"node_id\",\"node_target\"])\n",
    "\n",
    "labelled_data = labelled_data.replace(r'^\\s*$', \"others\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labelled_data.node_target.value_counts().reset_index().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val = model_selection.train_test_split(labelled_data, random_state = 42,\n",
    "                                            train_size = 0.8, stratify = labelled_data.node_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the targets\n",
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "train_targets = target_encoding.fit_transform(train.node_target)\n",
    "val_targets = target_encoding.fit_transform(val.node_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FullBatchNodeGenerator(G_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flow = generator.flow(train.node_id, train_targets)\n",
    "val_flow = generator.flow(val.node_id, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Formation\n",
    "# two layers of GCN\n",
    "gcn = GCN(layer_sizes=[8, 4], activations=[\"selu\", \"selu\"], generator=generator, dropout=0.5)\n",
    "# expose in and out to create keras model\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "# usual output layer\n",
    "predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)\n",
    "\n",
    "# define model\n",
    "model = Model(inputs=x_inp, outputs=predictions)\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es_callback = EarlyStopping(monitor=\"val_auc\", patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_flow,\n",
    "    epochs=50,\n",
    "    validation_data=val_flow,\n",
    "    verbose=2,\n",
    "    callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with class imbalance corrected\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(labelled_data.node_target),\n",
    "                                                 labelled_data.node_target)\n",
    "assert all(target_encoding.classes_ == np.unique(labelled_data.node_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels, label_encoder.classes_\n",
    "\n",
    "labels_encoded, classes = encode_label(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
